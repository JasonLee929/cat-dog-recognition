import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms, models
from torch.utils.data import DataLoader
from torch.optim.lr_scheduler import StepLR
# sklearnç›¸å…³ï¼ˆSVMè®­ç»ƒ+æŒ‡æ ‡è®¡ç®—ï¼‰
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix
from sklearn.preprocessing import StandardScaler
# å¯è§†åŒ–ç›¸å…³
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from sklearn.decomposition import PCA  # ç”¨äºç‰¹å¾é™ç»´å¯è§†åŒ–

warnings.filterwarnings('ignore')  # å±è”½æ— å…³è­¦å‘Š

# ===================== 1. æ ¸å¿ƒé…ç½®ï¼ˆé€‚é…ä½ çš„æ•°æ®é›†è·¯å¾„ï¼‰ =====================
# è®­ç»ƒ/æµ‹è¯•æ•°æ®é›†æ ¹è·¯å¾„ï¼ˆWindowsç»å¯¹è·¯å¾„ï¼Œç”¨åŸå§‹å­—ç¬¦ä¸²é¿å…è½¬ä¹‰ï¼‰
TRAIN_ROOT = r'C:\Users\32431\Desktop\Maâ€˜s\å…¥é—¨ä»»åŠ¡\2çŒ«ç‹—è¯†åˆ«ç²¾å‡†æ•°æ®åº“\training_data'
TEST_ROOT = r'C:\Users\32431\Desktop\Maâ€˜s\å…¥é—¨ä»»åŠ¡\2çŒ«ç‹—è¯†åˆ«ç²¾å‡†æ•°æ®åº“\testing_data'
# ç±»åˆ«åç§°ï¼ˆä¸æ•°æ®é›†æ–‡ä»¶å¤¹åå¯¹åº”ï¼‰
CLASSES = ['cats', 'dogs']
# è®­ç»ƒè¶…å‚æ•°
BATCH_SIZE = 32
NUM_EPOCHS = 10
LEARNING_RATE = 0.001
DEVICE = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")  # è‡ªåŠ¨é€‚é…CPU/GPU

# ===================== 2. æ•°æ®é¢„å¤„ç†ä¸åŠ è½½ï¼ˆWindowså…¼å®¹ï¼‰ =====================
data_transforms = {
    'train': transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation(10),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'test': transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
}

# åŠ è½½æ•°æ®é›†
image_datasets = {
    'train': datasets.ImageFolder(root=TRAIN_ROOT, transform=data_transforms['train']),
    'test': datasets.ImageFolder(root=TEST_ROOT, transform=data_transforms['test'])
}

dataloaders = {
    'train': DataLoader(image_datasets['train'], batch_size=BATCH_SIZE, shuffle=True, num_workers=0),
    'test': DataLoader(image_datasets['test'], batch_size=BATCH_SIZE, shuffle=False, num_workers=0)
}

dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}
print(f"è®­ç»ƒé›†æ ·æœ¬æ•°ï¼š{dataset_sizes['train']} | æµ‹è¯•é›†æ ·æœ¬æ•°ï¼š{dataset_sizes['test']}")
print(f"ä½¿ç”¨è®¾å¤‡ï¼š{DEVICE}")


# ===================== 3. æ¨¡å‹å®šä¹‰ï¼ˆResNet18/Swin-T/SVMï¼‰ =====================
def get_model(model_name='resnet', num_classes=2, use_pretrained=True):
    """è·å–æŒ‡å®šç±»å‹çš„æ¨¡å‹"""

    if model_name == 'resnet':
        # ResNet18æ¨¡å‹
        weights = 'DEFAULT' if use_pretrained else None
        model = models.resnet18(weights=weights)
        num_ftrs = model.fc.in_features
        model.fc = nn.Linear(num_ftrs, num_classes)

    elif model_name == 'swin':
        # Swin-Tæ¨¡å‹
        try:
            weights = 'DEFAULT' if use_pretrained else None
            model = models.swin_t(weights=weights)
            num_ftrs = model.head.in_features
            model.head = nn.Linear(num_ftrs, num_classes)
        except AttributeError:
            print("è­¦å‘Šï¼šå½“å‰torchvisionç‰ˆæœ¬ä¸æ”¯æŒswin_tæ¨¡å‹ï¼Œå°†ä½¿ç”¨ResNet18æ›¿ä»£")
            return get_model('resnet', num_classes, use_pretrained)

    elif model_name == 'svm':
        # SVMæ¨¡å‹ï¼ˆè¿”å›Noneï¼Œå› ä¸ºSVMåœ¨sklearnä¸­å•ç‹¬è®­ç»ƒï¼‰
        return None

    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ï¼š{model_name}ï¼Œå¯é€‰'resnet'/'swin'/'svm'")

    return model.to(DEVICE)


# ===================== 4. PyTorchæ¨¡å‹è®­ç»ƒå¾ªç¯ =====================
def train_model(model, dataloaders, criterion, optimizer, scheduler, num_epochs=10):
    """è®­ç»ƒPyTorchæ¨¡å‹"""
    best_acc = 0.0
    best_model_wts = model.state_dict()

    for epoch in range(num_epochs):
        print(f'\nEpoch {epoch + 1}/{num_epochs}')
        print('-' * 50)

        for phase in ['train', 'test']:
            if phase == 'train':
                model.train()
            else:
                model.eval()

            running_loss = 0.0
            running_corrects = 0

            for inputs, labels in dataloaders[phase]:
                inputs = inputs.to(DEVICE)
                labels = labels.to(DEVICE)

                optimizer.zero_grad()

                with torch.set_grad_enabled(phase == 'train'):
                    outputs = model(inputs)
                    _, preds = torch.max(outputs, 1)
                    loss = criterion(outputs, labels)

                    if phase == 'train':
                        loss.backward()
                        optimizer.step()

                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)

            if phase == 'train':
                scheduler.step()

            epoch_loss = running_loss / dataset_sizes[phase]
            epoch_acc = running_corrects.double() / dataset_sizes[phase]

            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')

            if phase == 'test' and epoch_acc > best_acc:
                best_acc = epoch_acc
                best_model_wts = model.state_dict()

    model.load_state_dict(best_model_wts)
    print(f'\næœ€ä½³æµ‹è¯•ç²¾åº¦: {best_acc:.4f}')
    return model


# ===================== 5. SVMç›¸å…³åŠŸèƒ½ =====================
def get_features_for_svm(dataloader, model_type='resnet', use_pretrained=True):
    """æå–å›¾åƒç‰¹å¾ç”¨äºSVMè®­ç»ƒ"""

    # åŠ è½½ç‰¹å¾æå–æ¨¡å‹
    if model_type == 'resnet':
        model = models.resnet18(weights='DEFAULT' if use_pretrained else None)
        # ç§»é™¤æœ€åä¸€å±‚å…¨è¿æ¥å±‚
        feature_extractor = torch.nn.Sequential(*list(model.children())[:-1])
    elif model_type == 'swin':
        model = models.swin_t(weights='DEFAULT' if use_pretrained else None)
        # ç§»é™¤åˆ†ç±»å¤´
        feature_extractor = torch.nn.Sequential(*list(model.children())[:-1])
    else:
        raise ValueError("ä»…æ”¯æŒresnetæˆ–swinä½œä¸ºç‰¹å¾æå–å™¨")

    feature_extractor = feature_extractor.to(DEVICE)
    feature_extractor.eval()

    features = []
    labels = []

    with torch.no_grad():
        for inputs, target in dataloader:
            inputs = inputs.to(DEVICE)
            output = feature_extractor(inputs)
            output = output.view(output.size(0), -1)  # å±•å¹³ç‰¹å¾
            features.append(output.cpu().numpy())
            labels.append(target.numpy())

    return np.concatenate(features), np.concatenate(labels)


def train_svm(train_features, train_labels, test_features, test_labels, kernel='rbf'):
    """è®­ç»ƒå¹¶è¯„ä¼°SVMæ¨¡å‹"""

    # ç‰¹å¾æ ‡å‡†åŒ–
    scaler = StandardScaler()
    train_features_scaled = scaler.fit_transform(train_features)
    test_features_scaled = scaler.transform(test_features)

    # è®­ç»ƒSVM
    svm_model = SVC(kernel=kernel, C=1.0, gamma='scale', probability=True)
    svm_model.fit(train_features_scaled, train_labels)

    # é¢„æµ‹
    train_preds = svm_model.predict(train_features_scaled)
    test_preds = svm_model.predict(test_features_scaled)

    # è®¡ç®—æŒ‡æ ‡
    train_acc = accuracy_score(train_labels, train_preds)
    test_acc = accuracy_score(test_labels, test_preds)
    test_prec = precision_score(test_labels, test_preds, average='macro')
    test_rec = recall_score(test_labels, test_preds, average='macro')
    test_cm = confusion_matrix(test_labels, test_preds)

    print(f"SVMè®­ç»ƒé›†å‡†ç¡®ç‡: {train_acc:.4f}")
    print(f"SVMæµ‹è¯•é›†å‡†ç¡®ç‡: {test_acc:.4f}")
    print(f"SVMæµ‹è¯•é›†ç²¾ç¡®ç‡: {test_prec:.4f}")
    print(f"SVMæµ‹è¯•é›†å¬å›ç‡: {test_rec:.4f}")

    return svm_model, test_acc, test_prec, test_rec, test_cm, test_preds


# ===================== 6. ç»“æœåˆ†æä¸å¯è§†åŒ– =====================
def plot_results(model_name, cm, acc, prec, rec, wrong_samples=None):
    """ç»˜åˆ¶ç»“æœå¯è§†åŒ–"""

    # 1. ç»˜åˆ¶æ··æ·†çŸ©é˜µ
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=CLASSES, yticklabels=CLASSES)
    plt.title(f'{model_name} - æ··æ·†çŸ©é˜µ (Acc={acc:.4f}, Prec={prec:.4f}, Rec={rec:.4f})')
    plt.ylabel('çœŸå®æ ‡ç­¾')
    plt.xlabel('é¢„æµ‹æ ‡ç­¾')
    plt.tight_layout()
    plt.savefig(f'{model_name}_confusion_matrix.png')
    plt.show()

    # 2. å¯è§†åŒ–é”™è¯¯è¯†åˆ«æ ·æœ¬
    if wrong_samples and len(wrong_samples) > 0:
        plt.figure(figsize=(15, 6))
        num_samples = min(5, len(wrong_samples))
        for i in range(num_samples):
            img, pred, true = wrong_samples[i]
            plt.subplot(1, num_samples, i + 1)
            # åå½’ä¸€åŒ–
            img = img.permute(1, 2, 0)
            mean = torch.tensor([0.485, 0.456, 0.406])
            std = torch.tensor([0.229, 0.224, 0.225])
            img = img * std + mean
            img = torch.clamp(img, 0, 1)
            plt.imshow(img.numpy())
            plt.title(f'Pred: {CLASSES[pred]}\nTrue: {CLASSES[true]}')
            plt.axis('off')
        plt.suptitle(f'{model_name} - é”™è¯¯è¯†åˆ«æ ·æœ¬')
        plt.tight_layout()
        plt.savefig(f'{model_name}_wrong_samples.png')
        plt.show()


def plot_feature_space(train_features, train_labels, test_features, test_labels, model_name):
    """å¯è§†åŒ–ç‰¹å¾ç©ºé—´"""
    # ä½¿ç”¨PCAé™ç»´åˆ°2D
    pca = PCA(n_components=2)

    # åˆå¹¶è®­ç»ƒå’Œæµ‹è¯•ç‰¹å¾
    all_features = np.vstack([train_features, test_features])
    all_labels = np.concatenate([train_labels, test_labels])

    # åº”ç”¨PCA
    features_2d = pca.fit_transform(all_features)

    # åˆ†å¼€è®­ç»ƒå’Œæµ‹è¯•
    train_features_2d = features_2d[:len(train_labels)]
    test_features_2d = features_2d[len(train_labels):]

    plt.figure(figsize=(12, 5))

    # è®­ç»ƒé›†ç‰¹å¾
    plt.subplot(1, 2, 1)
    for i, class_name in enumerate(CLASSES):
        idx = train_labels == i
        plt.scatter(train_features_2d[idx, 0], train_features_2d[idx, 1],
                    alpha=0.6, label=class_name, s=20)
    plt.title(f'{model_name} - è®­ç»ƒé›†ç‰¹å¾ç©ºé—´(PCA)')
    plt.xlabel('PCA Component 1')
    plt.ylabel('PCA Component 2')
    plt.legend()

    # æµ‹è¯•é›†ç‰¹å¾
    plt.subplot(1, 2, 2)
    for i, class_name in enumerate(CLASSES):
        idx = test_labels == i
        plt.scatter(test_features_2d[idx, 0], test_features_2d[idx, 1],
                    alpha=0.6, label=class_name, s=20)
    plt.title(f'{model_name} - æµ‹è¯•é›†ç‰¹å¾ç©ºé—´(PCA)')
    plt.xlabel('PCA Component 1')
    plt.ylabel('PCA Component 2')
    plt.legend()

    plt.tight_layout()
    plt.savefig(f'{model_name}_feature_space.png')
    plt.show()


# ===================== 7. æ¨¡å‹è¯„ä¼° =====================
def evaluate_model(model, dataloader, device, classes, model_type='pytorch'):
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""

    if model_type == 'svm':
        # SVMè¯„ä¼°å·²ç»åœ¨train_svmå‡½æ•°ä¸­å®Œæˆ
        return None, None, None, None, []

    model.eval()
    all_preds = []
    all_labels = []
    wrong_identifications = []

    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs = inputs.to(device)

            if model_type == 'pytorch':
                outputs = model(inputs)
                _, preds = torch.max(outputs, 1)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")

            preds_cpu = preds.cpu().numpy()
            labels_cpu = labels.cpu().numpy()

            all_preds.extend(preds_cpu)
            all_labels.extend(labels_cpu)

            # æ”¶é›†é”™è¯¯æ ·æœ¬
            if len(wrong_identifications) < 5:
                wrong_mask = preds_cpu != labels_cpu
                for i in range(min(5, sum(wrong_mask))):
                    idx = np.where(wrong_mask)[0][i]
                    wrong_identifications.append((
                        inputs[idx].cpu(),
                        preds_cpu[idx],
                        labels_cpu[idx]
                    ))

    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    acc = accuracy_score(all_labels, all_preds)
    prec = precision_score(all_labels, all_preds, average='macro')
    rec = recall_score(all_labels, all_preds, average='macro')
    cm = confusion_matrix(all_labels, all_preds)

    return acc, prec, rec, cm, wrong_identifications


# ===================== 8. ä¸»å‡½æ•° - è¿è¡Œä¸‰ä¸ªæ¨¡å‹ =====================
def main():
    """ä¸»å‡½æ•°ï¼šä¾æ¬¡è¿è¡Œä¸‰ä¸ªæ¨¡å‹"""

    results = {}  # å­˜å‚¨æ‰€æœ‰æ¨¡å‹çš„ç»“æœ

    # -------------------- 1. è®­ç»ƒå’Œè¯„ä¼°ResNet18 --------------------
    print("\n" + "=" * 60)
    print("é˜¶æ®µ1ï¼šè®­ç»ƒå’Œè¯„ä¼°ResNet18æ¨¡å‹")
    print("=" * 60)

    resnet_model = get_model('resnet', num_classes=2, use_pretrained=True)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(resnet_model.parameters(), lr=LEARNING_RATE)
    scheduler = StepLR(optimizer, step_size=5, gamma=0.1)

    # è®­ç»ƒæ¨¡å‹
    resnet_model = train_model(resnet_model, dataloaders, criterion, optimizer,
                               scheduler, num_epochs=NUM_EPOCHS)

    # ä¿å­˜æ¨¡å‹
    torch.save(resnet_model.state_dict(), 'best_resnet18.pth')

    # è¯„ä¼°æ¨¡å‹
    acc, prec, rec, cm, wrong_samples = evaluate_model(
        resnet_model, dataloaders['test'], DEVICE, CLASSES, 'pytorch'
    )

    results['ResNet18'] = {
        'accuracy': acc,
        'precision': prec,
        'recall': rec,
        'confusion_matrix': cm
    }

    print(f"ResNet18 æµ‹è¯•é›†æŒ‡æ ‡ï¼š")
    print(f"å‡†ç¡®ç‡: {acc:.4f} | ç²¾ç¡®ç‡: {prec:.4f} | å¬å›ç‡: {rec:.4f}")

    # å¯è§†åŒ–ç»“æœ
    plot_results('ResNet18', cm, acc, prec, rec, wrong_samples)

    # -------------------- 2. è®­ç»ƒå’Œè¯„ä¼°SVM --------------------
    print("\n" + "=" * 60)
    print("é˜¶æ®µ2ï¼šä½¿ç”¨ResNet18ç‰¹å¾è®­ç»ƒSVM")
    print("=" * 60)

    # æå–ç‰¹å¾
    print("æå–è®­ç»ƒé›†ç‰¹å¾...")
    train_features, train_labels = get_features_for_svm(dataloaders['train'], 'resnet', True)
    print("æå–æµ‹è¯•é›†ç‰¹å¾...")
    test_features, test_labels = get_features_for_svm(dataloaders['test'], 'resnet', True)

    # è®­ç»ƒSVM
    svm_model, svm_acc, svm_prec, svm_rec, svm_cm, svm_preds = train_svm(
        train_features, train_labels, test_features, test_labels, 'rbf'
    )

    results['SVM'] = {
        'accuracy': svm_acc,
        'precision': svm_prec,
        'recall': svm_rec,
        'confusion_matrix': svm_cm
    }

    # å¯è§†åŒ–SVMç‰¹å¾ç©ºé—´
    plot_feature_space(train_features, train_labels, test_features, test_labels, 'SVM')
    plot_results('SVM', svm_cm, svm_acc, svm_prec, svm_rec)

    # -------------------- 3. è®­ç»ƒå’Œè¯„ä¼°Swin-T --------------------
    print("\n" + "=" * 60)
    print("é˜¶æ®µ3ï¼šè®­ç»ƒå’Œè¯„ä¼°Swin-Tæ¨¡å‹")
    print("=" * 60)

    swin_model = get_model('swin', num_classes=2, use_pretrained=True)

    if swin_model is not None:
        swin_criterion = nn.CrossEntropyLoss()
        swin_optimizer = optim.Adam(swin_model.parameters(), lr=LEARNING_RATE)
        swin_scheduler = StepLR(swin_optimizer, step_size=5, gamma=0.1)

        # è®­ç»ƒSwin-T
        swin_model = train_model(swin_model, dataloaders, swin_criterion,
                                 swin_optimizer, swin_scheduler, num_epochs=NUM_EPOCHS)

        # ä¿å­˜æ¨¡å‹
        torch.save(swin_model.state_dict(), 'best_swin_t.pth')

        # è¯„ä¼°æ¨¡å‹
        swin_acc, swin_prec, swin_rec, swin_cm, swin_wrong = evaluate_model(
            swin_model, dataloaders['test'], DEVICE, CLASSES, 'pytorch'
        )

        results['Swin-T'] = {
            'accuracy': swin_acc,
            'precision': swin_prec,
            'recall': swin_rec,
            'confusion_matrix': swin_cm
        }

        print(f"Swin-T æµ‹è¯•é›†æŒ‡æ ‡ï¼š")
        print(f"å‡†ç¡®ç‡: {swin_acc:.4f} | ç²¾ç¡®ç‡: {swin_prec:.4f} | å¬å›ç‡: {swin_rec:.4f}")

        # å¯è§†åŒ–ç»“æœ
        plot_results('Swin-T', swin_cm, swin_acc, swin_prec, swin_rec, swin_wrong)
    else:
        print("Swin-Tæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè·³è¿‡æ­¤æ¨¡å‹")
        results['Swin-T'] = None

    # -------------------- 4. ç»“æœå¯¹æ¯” --------------------
    print("\n" + "=" * 60)
    print("é˜¶æ®µ4ï¼šä¸‰ç§æ¨¡å‹æ€§èƒ½å¯¹æ¯”")
    print("=" * 60)

    print(f"\n{'æ¨¡å‹':<15} {'å‡†ç¡®ç‡':<12} {'ç²¾ç¡®ç‡':<12} {'å¬å›ç‡':<12}")
    print("-" * 60)

    for model_name, metrics in results.items():
        if metrics is not None:
            print(f"{model_name:<15} {metrics['accuracy']:.4f}      "
                  f"{metrics['precision']:.4f}      {metrics['recall']:.4f}")

    # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
    if results['Swin-T'] is not None:
        valid_results = results
    else:
        valid_results = {k: v for k, v in results.items() if v is not None}

    best_model = max(valid_results, key=lambda x: valid_results[x]['accuracy'])
    print("\n" + "=" * 60)
    print(f"ğŸ† æœ€ä½³æ¨¡å‹: {best_model}, å‡†ç¡®ç‡: {valid_results[best_model]['accuracy']:.4f}")
    print("=" * 60)

    # ä¿å­˜æ‰€æœ‰ç»“æœåˆ°æ–‡ä»¶
    import json
    with open('model_results.json', 'w') as f:
        # å°†numpyæ•°ç»„è½¬æ¢ä¸ºåˆ—è¡¨ä»¥ä¾¿JSONåºåˆ—åŒ–
        serializable_results = {}
        for model_name, metrics in results.items():
            if metrics is not None:
                serializable_results[model_name] = {
                    'accuracy': float(metrics['accuracy']),
                    'precision': float(metrics['precision']),
                    'recall': float(metrics['recall']),
                    'confusion_matrix': metrics['confusion_matrix'].tolist()
                }
            else:
                serializable_results[model_name] = None
        json.dump(serializable_results, f, indent=4)

    print("\nâœ… æ‰€æœ‰æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
    print("ğŸ“Š ç»“æœå·²ä¿å­˜åˆ°: model_results.json")
    print("ğŸ“¸ å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜ä¸ºPNGæ–‡ä»¶")


if __name__ == '__main__':
    main()